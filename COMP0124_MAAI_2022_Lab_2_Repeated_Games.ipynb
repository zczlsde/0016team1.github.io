{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zczlsde/0016team1.github.io/blob/gh-pages/COMP0124_MAAI_2022_Lab_2_Repeated_Games.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgf7mqEmqDz1"
      },
      "source": [
        "# COMP0124 MAAI Lab 2 Repeated Games\n",
        "\n",
        "## Instructions\n",
        "1.   To start this notebook, please duplicate this notebook at first:\n",
        "  - Choose \"File => Save a copy in Drive\" and open/run it in Colab.\n",
        "  - Or you can download the notebook and run it in your local jupyter notebook server.\n",
        "2.   For the coding assignment and practice, please write your code at `### TODO ###` blocks or in a new cell. For analysis report, you are free to use as many blocks as you need.\n",
        "3. If you have any questions, please contact TAs: [Minne Li](minne.li@cs.ucl.ac.uk), [Oliver Slumbers](o.slumbers@cs.ucl.ac.uk), [Xihan Li](xihan.li.20@ucl.ac.uk), [Xidong Feng](xidong.feng.20@cs.ucl.ac.uk), and [Mengyue Yang](m.yang@cs.ucl.ac.uk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6CZRs4VqbbC"
      },
      "source": [
        "## Part I: Repeated Games\n",
        "In a Repeated Game, a (simultaneous- move) normal form game is played over and over again by the same players.\n",
        "\n",
        "Suppose that two people are involved repeatedly in playing the Prinsoner's Dilemma game, with the payoff of the stage game (the one they play at each time step) as:\n",
        "\n",
        "$$\n",
        "\\mathbf{R}^1 = \\left[\\begin{matrix}\n",
        "2 & 0 \\\\\n",
        "3 & 1\n",
        "\\end{matrix}\\right] \n",
        "\\quad \n",
        "\\mathbf{R}^2 = \\left[\\begin{matrix}\n",
        "2 & 3 \\\\\n",
        "0 & 1\n",
        "\\end{matrix}\\right],\n",
        "$$\n",
        "\n",
        "where the action 0 (correpsonds to column/row 0) as \"cooperation\" (don't confess) and the action 1 (correpsonds to column/row 1) as \"defection\" (confess).\n",
        "\n",
        "Compared with the stage game, there are many new strategies in repeated games as players are able to condition their plays on the\n",
        "history of plays in previous rounds. The basic idea is that a player may be deterred from exploiting her short-term advantage by the \"threat\" of \"punishment\" that reduces her long-term payoff.\n",
        "\n",
        "We use a discount payoff function for an infinite repeated game. Player i’s total utility for history action $h = (a_0, a_1, a_2, ...)$ is\n",
        "\n",
        "$$\n",
        "u_{i}(h)=\\sum_{k=0}^{\\infty} \\delta^{k} u_{i}\\left(a^{k}\\right) \\tag{1}\n",
        "$$\n",
        "\n",
        "where $\\delta$ is the discount factor $(0 < \\delta < 1)$, $u_i$ is the agent i's utility function and $a^k =(a^k_1, a^k_2)$ is the set of actions for all the two players at time $k$.\n",
        "\n",
        "**Note:** For the implementation below, we create a repeated game with a finite-time-horizon (with max timestep = 100) and discount factor $\\delta=0.9$ to make sure $0.9^{100}$ is small enough to mitigate the discrepancy between infinite and finite setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNmdUiqRqw8Y"
      },
      "source": [
        "## Q1: Run simulations on iterated prinsoner dilemma to evaluate strateiges\n",
        "\n",
        "In this excersice, you will try to run simulations on Iterated Prinsoner Dilemma in order to evaluate multiple strategies. To achieve that, you will need to implement the following things:\n",
        "\n",
        "*   **Step I**: Develop an environment for Iterated Prisoner's Dilemma. In this environment you can set all the necessary parameters, and also some functions to enable the simulation step.\n",
        "*   **Step II**: Develop different strategies. You need to implement strategies which determine agents policy in the iterated game.\n",
        "*   **Step III**: Evaluation of two different strategies for discounted return. Given an environment and different strategies, you need to implement an evaluation function to evaluate the return between different strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4DR2sDOvAf3"
      },
      "source": [
        "### Step 1: An iterated prinsoner dilemma environment\n",
        "\n",
        "In this part, you will build an environment with 3 functions:\n",
        "* Complete the **step** function: input pure action for two agents and return 4 things: immediate reward for agent1, immediate reward for agent2, adding the two actions to the action history for both agents, and whether the environment step has reached the max_step (Done=True or Done=False).\n",
        "\n",
        "**Complete the code to build an environment.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LufVJwxp6Ee"
      },
      "source": [
        "import numpy as np\n",
        "class iterated_games():\n",
        "  def __init__(self, payoff1, payoff2, max_step):\n",
        "    self.payoff1 = payoff1\n",
        "    self.payoff2 = payoff2\n",
        "    self.max_step_num = max_step\n",
        "    self.reset()\n",
        "  def reset(self):\n",
        "    self.history = {'agent1':[], 'agent2':[]}\n",
        "    self.step_num = 0\n",
        "    return self.history\n",
        "  def step(self, a1, a2):\n",
        "    ### We recommend that the history can be a dictionary, where each element is a list\n",
        "    ### like self.history['agent1'] = [a11, a12, ...], self.history['agent2']=[a21, a22, ...] defined in the above function\n",
        "    ### So you can directly return self.history\n",
        "\n",
        "    ### input: a1 refers to action for agent 1, a2 refers to action for agent 2\n",
        "    ### TODO: Implement the step function ###\n",
        "\n",
        "    ### END TODO ###\n",
        "\n",
        "    return r1, r2, self.history, done\n",
        "    # return\n",
        "    # r1: agent1's reward\n",
        "    # r2: agent2's reward\n",
        "    # history: dictionary contains historical information\n",
        "    # done: if step_num reaches max step number, done=True, else done=False\n",
        "\n",
        "### Run some simulations to verify the correctness of your algorithm\n",
        "p1 = np.random.randn(2,2) # two random payoff matrix\n",
        "p2 = np.random.randn(2,2)\n",
        "env = iterated_games(p1, p2, 10)\n",
        "done = False\n",
        "print(p1)\n",
        "print(p2)\n",
        "while True:\n",
        "  if done:\n",
        "    break\n",
        "  a1 = np.random.choice(2)\n",
        "  a2 = np.random.choice(2)\n",
        "  r1, r2, history, done = env.step(a1, a2)\n",
        "  print(a1, a2, r1, r2,history,done)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTMeZ6rCoc3K"
      },
      "source": [
        "### Step 2: Implement different strategies\n",
        "In this part, you will try to implement 4 different strategies:\n",
        "\n",
        "* **Grim trigger strategy**: choose cooperation so long as the other player chooses cooperation, if in any period the other player chooses defection, then choose defection in every subsequent period\n",
        "* **Limited punishment strategy**: This strategy punishes deviations for $k$ periods. It responds to a deviation by choosing the action D\n",
        "for $k$ periods, then reverting to cooperation, no matter how the other player behaved during her punishment.\n",
        "* **Tit-for-tat strategy**: In strategy tit-for-tat, the length of the\n",
        "punishment depends on the behavior of the player being punished. If she continues to choose defection then tit-for-tat continues to do so. if she reverts to cooperation then tit-for-tat reverts to cooperation also.\n",
        "* Any strategy you prefer\n",
        "\n",
        "where these strategies will get action history dictionary and output the action. \n",
        "\n",
        "**Complete the code to implement these strategies.**\n",
        "\n",
        "**To give an example, Random strategy, Cooperation with probability 0.5 and defection with probability 0.5, has been given below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QKyhFgWpic4"
      },
      "source": [
        "### We define the idx for identifying agent because the step function gets the history for both agents\n",
        "### Or you can implement agent in your own ways\n",
        "### Note that you need to handle corner case when the length of history is 0\n",
        "\n",
        "### All agent get input of history dictionary, output the action\n",
        "\n",
        "class random_agent:\n",
        "  def __init__(self, idx=0):\n",
        "    self.idx = idx ### idx for identifying which agent\n",
        "  def step(self, history_both):\n",
        "    action = np.random.choice(2)  \n",
        "    return action\n",
        "\n",
        "class grim_trigger_agent:\n",
        "  def __init__(self, idx=0):\n",
        "    self.idx = idx\n",
        "  def step(self, history_both):\n",
        "    if self.idx == 0:\n",
        "      history = history_both['agent2'] # use other_agent history to determine the action\n",
        "    else:\n",
        "      history = history_both['agent1'] # use other_agent history to determine the action\n",
        "    ### TODO: Implement grim trigger agent ###\n",
        "    \n",
        "    ### END TODO ###\n",
        "    return action\n",
        "\n",
        "class limited_punish_agent:\n",
        "  def __init__(self, k=3, idx=0):\n",
        "    self.k = k # punishment step\n",
        "    self.idx = idx\n",
        "    ### TODO: Add the variables/functions if you need  ###\n",
        "   \n",
        "    ### END TODO  ###\n",
        "  def step(self, history_both):\n",
        "    if self.idx == 0:\n",
        "      history = history_both['agent2'] # use other_agent history to determine the action\n",
        "    else:\n",
        "      history = history_both['agent1'] # use other_agent history to determine the action\n",
        "    ### TODO: Implement limited punishment agent ###\n",
        "    ### Note you may need to handle many corner cases:\n",
        "    ### when the agent reverts back to cooperation: have to take cooperation no matter how the other player behaved\n",
        "    ### when the agent is inside the k punishment step\n",
        "    ### when the agent begin to punish\n",
        "   \n",
        "    ### END TODO  ###\n",
        "    return action\n",
        "\n",
        "class tit_for_tat_agent:\n",
        "  def __init__(self, idx=0):\n",
        "    self.idx = idx\n",
        "  def step(self, history_both):\n",
        "    if self.idx == 0:\n",
        "      history = history_both['agent2'] # use other_agent history to determine the action\n",
        "    else:\n",
        "      history = history_both['agent1'] # use other_agent history to determine the action\n",
        "    ### TODO: Implement tit for tat ###\n",
        "   \n",
        "    ### END TODO  ###\n",
        "    return action\n",
        "\n",
        "class self_designed_agent:\n",
        "  def __init__(self, idx=0):\n",
        "    self.idx = idx\n",
        "  def step(self, history_both):\n",
        "    ### TODO: Implement self-design agent ###\n",
        "   \n",
        "    ### END TODO  ###\n",
        "    return action\n",
        "\n",
        "### Run some simulations to verify the correctness of your algorithm\n",
        "### Random Agent\n",
        "history = {'agent1': [1, 0, 0], 'agent2': [1, 0, 1]}\n",
        "agent = random_agent(idx=0)\n",
        "print('Test random agent')\n",
        "for _ in range(5):\n",
        "  print(agent.step(history))\n",
        "\n",
        "### grim_trigger_agent for testing 2 cases\n",
        "history = {'agent1': [1, 0, 0], 'agent2': [0, 0, 0]}\n",
        "agent = grim_trigger_agent(idx=0)\n",
        "print('Test grim_trigger_agent, case 1')\n",
        "print('it should be', 0, agent.step(history))\n",
        "\n",
        "history = {'agent1': [1, 0, 0], 'agent2': [0, 1, 0]}\n",
        "print('Test grim_trigger_agent, case 2')\n",
        "print('it should be', 1, agent.step(history))\n",
        "\n",
        "### limited_punish_agent for testing\n",
        "### You might define some flag variables in the implementation of limited_punish_agent\n",
        "### So here it might be hard to directly test the correctness\n",
        "### You might need to firstly implement the evaluation function in the next section \n",
        "### Then use that to verify the correctness\n",
        "\n",
        "\n",
        "### tit_for_tat_agent for testing 2 cases\n",
        "history = {'agent1': [1, 0, 0], 'agent2': [0, 1, 0]}\n",
        "agent = tit_for_tat_agent(idx=0)\n",
        "print('Test tit_for_tat_agent, case 1')\n",
        "print('it should be', 0, agent.step(history))\n",
        "\n",
        "history = {'agent1': [1, 0, 0], 'agent2': [0, 1, 1]}\n",
        "print('Test tit_for_tat_agent, case 2')\n",
        "print('it should be', 1, agent.step(history))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSpWJfS2qPsW"
      },
      "source": [
        "### Step 3: Evaluation the results of playing repeated games\n",
        "\n",
        "In this part, you will implement the following things:\n",
        "* **get_discount_return** function: implement the calculation of discounted accumulated returns (rewards) defined in Equation (1) given the list of received rewards and discount factor $\\delta$.\n",
        "* Complete **evaluate** function that get rollout with two strategies in the iteated Prisoner's Dilemma. This function will finally return two returns for these two strategies.\n",
        "* Try to conduct pairwise policy evaluation among 5 strategies defined in Step 2 and print out the 5$\\times$5 return matrix for agent 1.(Note that for stochastic strategies evaluation you may need to run many times to get the average)\n",
        "\n",
        "**Complete the code to evaluate these strategies.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi9i1hNTwkQe"
      },
      "source": [
        "def get_discount_return(return_list, delta):\n",
        "  return_all = np.array(return_list)\n",
        "  ### TODO: Implement discount return calculation ###\n",
        "\n",
        "  ### END TODO  ###\n",
        "  return return_all\n",
        "\n",
        "def evaluate(env, agent1, agent2, delta):\n",
        "  history = env.reset()\n",
        "  done = False\n",
        "  r1_list = []\n",
        "  r2_list = []\n",
        "  while True:\n",
        "    if done:\n",
        "      break\n",
        "    ### TODO: Implement rollouts ### \n",
        "    ### First take actions for both agent given the history\n",
        "    ### Then take env rollout by env.step based on these two actions\n",
        "    ### Finally, store the reward in r1_list, r2_list\n",
        "\n",
        "    ### END TODO  ###\n",
        "  return1 = get_discount_return(r1_list, delta) # Discounted return for the first policy\n",
        "  return2 = get_discount_return(r2_list, delta) # Discounted return for the second policy\n",
        "  return return1, return2\n",
        "\n",
        "payoff1 = np.array([[2,0],[3,1]])\n",
        "payoff2 = np.array([[2,3],[0,1]])\n",
        "delta = 0.9\n",
        "max_step = 100\n",
        "env = iterated_games(payoff1, payoff2, max_step)\n",
        "\n",
        "payoff_matrix = np.zeros([5,5])\n",
        "### TODO: Conduct pair-wise strategy comparison among these 5 strategies and get the final 5*5 marix (Run multiple simulations for stochastic strategy)###\n",
        "\n",
        "### END TODO  ###\n",
        "print(payoff_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbYS_O-Bd_eQ"
      },
      "source": [
        "## Q2: Nash Equilibrium condition for strategies\n",
        "In this part, the aim is to study the relationship between δ the discount and the payoff function, by which you can understand the conditions for those strategies to be a Nash Equilibrium.\n",
        "\n",
        "* Verifying conditions of being a Nash Equilibrium for limited punishment strategy\n",
        "\n",
        "* Verifying conditions of being a Nash Equilibrium for tic for tat strategy\n",
        "\n",
        "**complete the code to visualise the Nash Equilibrium condtion for strategies.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFZCABYlXcTy"
      },
      "source": [
        "### Verifying conditions of being Nash Equilibrium for limited punishment strategy\n",
        "\n",
        "In the lecture, we know that there exists some conditions for limited punishment strategy to become Nash Equilibrium. In the sectoin, you need to  verify the analytical analysis with simulated results. \n",
        "\n",
        "The analytical non-deviation condition for limited punishment strategy(k) can be derived as:\n",
        "$$\n",
        "\\delta^{k+1}-2 \\delta+1 \\leq 0\n",
        "$$\n",
        "* if $k=1$, no value of $\\delta$ less than 1 satisfies the\n",
        "inequality\n",
        "* If $k=2$ then the inequality is satisfied for $\\delta \\geq 0.62$\n",
        "* As $k$ increases the lower bound on $\\delta$ approaches 0.5\n",
        "\n",
        "You will try to iterate k from 0 to 10. For each k, let's fix agent 1 policy as limited punishment strategy(k), please find out the corresponding $\\delta$ lower bound for limited punishment strategy to have better performance compared with the following two policies. Refer to P22/P23 in the slide. You need to implement:\n",
        "\n",
        "* **K-defection policy**: Complete the **first_k_d** class. Implement One policy that chooses defection in the first k rounds, while chooses cooperation in the following rounds\n",
        "* **All-cooperation policy**: Complete the **all_c** class. Implement One policy that chooses cooperation all the time\n",
        "* Get the relationship between $\\delta$ bound and k\n",
        "\n",
        "**Implemenatation Request**: Use simulation to generate the lower bound rather than analytical solution. You can just gradually increase $\\delta$ from 0.01 to 0.99 to find out the when all-cooperation policy can achieve better rewards compared with k-defection policy.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkYa7Wpa0bzH"
      },
      "source": [
        "payoff1 = np.array([[2,0],[3,1]])\n",
        "payoff2 = np.array([[2,3],[0,1]])\n",
        "max_step = 100\n",
        "env = iterated_games(payoff1, payoff2, max_step)\n",
        "\n",
        "class first_k_d:\n",
        "  def __init__(self, idx, k):\n",
        "    self.max_d_step = k\n",
        "    self.idx = idx\n",
        "    self.step_num = 0\n",
        "  def step(self, history_both):\n",
        "    ### TODO: Implement the agent  ###\n",
        "\n",
        "    ### END TODO  ###\n",
        "    return action\n",
        "\n",
        "class all_c:\n",
        "  def __init__(self, idx):\n",
        "    self.idx = idx\n",
        "  def step(self, history_both):\n",
        "    ## TODO: Implement the agent  ###\n",
        "\n",
        "    ### END TODO  ###\n",
        "    return action\n",
        "\n",
        "delta_list = np.linspace(0.01, 0.99, 50)\n",
        "k_list = np.array([i for i in range(1,10)])\n",
        "delta_list_calc = []\n",
        "\n",
        "for k in k_list:\n",
        "  for delta in delta_list:\n",
        "    # TODO: Iterate the whole k_list and delta_list, to find out the relationship bettwen delta bound and k ###\n",
        "    ### Compare the performance between (limited_punish_agent(k), first_k_d(k+1)), (limited_punish_agent(k), all_c)\n",
        "    ### it's k+1 because the first action for limited_punish_agent is C while it's still D for first_k_d\n",
        "    ### break the loop when all_c can achieve higher return than first_k_d\n",
        "\n",
        "    ### END TODO  ###\n",
        "  delta_list_calc.append(delta)\n",
        "\n",
        "### Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(k_list, delta_list_calc)\n",
        "plt.xlabel('K for Limited punishment strategy')\n",
        "plt.ylabel('Lower Bound Delta')\n",
        "plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9aG6MaD6v5"
      },
      "source": [
        "### Verifying conditions of being Nash Equilibrium for tit-for-tat strategy\n",
        "\n",
        "In the lecture, we have studied that there exist some conditions for the tit-for-tat strategy to become a Nash Equilibrium. In the section, you need to  verify the conclusions with simulated results. \n",
        "\n",
        "The analytical solution is $\\delta \\geq 0.5$. Check P25 in the repeatd game slide for detailed equations. By simulation, we need to derive the relationship between $\\delta$ and the return gained for **Alternation policy**/**All-defection policy**/**All-cooperation policy** to see when All-cooperation policy can achieve the best return.\n",
        "\n",
        "You need to implement:\n",
        "\n",
        "* **Alternation policy**: Complete the **alternate** class. Implement one policy that chooses defection and cooperation alternatively. (DCDCDCD...)\n",
        "* **All-defection policy**: Complete the **all_d** class. Implement one policy that chooses defection all the time\n",
        "* Visualise when All-cooperation policy can achieve the better return compared with the rest two policies. \n",
        "\n",
        "**Implementation Request**: Don't use analytical solution, You can just iterate the delta list from 0.01 to 0.8 and store the reward in corresponding list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCpIxQK6GAEJ"
      },
      "source": [
        "payoff1 = np.array([[2,0],[3,1]])\n",
        "payoff2 = np.array([[2,3],[0,1]])\n",
        "max_step = 100\n",
        "env = iterated_games(payoff1, payoff2, max_step)\n",
        "\n",
        "class alternate:\n",
        "  def __init__(self, idx):\n",
        "    self.idx = idx\n",
        "    self.step_num = 0\n",
        "  def step(self, history_both):\n",
        "    ### TODO: Implement the alternate agent  ###\n",
        "\n",
        "    ### END TODO  ###\n",
        "    return action\n",
        "\n",
        "class all_d:\n",
        "  def __init__(self, idx):\n",
        "    self.idx = idx\n",
        "  def step(self, history_both):\n",
        "    ### TODO: Implement the defection agent  ###\n",
        "\n",
        "    ### END TODO  ###\n",
        "    return action\n",
        "\n",
        "delta_list = np.linspace(0.01, 0.8, 50)\n",
        "reward_alterate = []\n",
        "reward_all_d = []\n",
        "reward_all_c = []\n",
        "\n",
        "for delta in delta_list:\n",
        "  ### TODO: Implement the agent  ###\n",
        "  ### Compare the performance between (tit_for_tat_agent, alternate), (tit_for_tat_agent, all_d) and (tit_for_tat_agent, all_c)\n",
        "\n",
        "  ### END TODO  ###\n",
        "\n",
        "  reward_alterate.append(r2)\n",
        "  reward_all_d.append(r2_1)\n",
        "  reward_all_c.append(r2_2)\n",
        "\n",
        "reward_alterate = np.array(reward_alterate)\n",
        "reward_all_d = np.array(reward_all_d)\n",
        "reward_all_c = np.array(reward_all_c)\n",
        "\n",
        "### Visualise the difference\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(delta_list, reward_alterate - reward_all_c, label='alternate')\n",
        "plt.plot(delta_list, reward_all_d - reward_all_c, label='all_defection')\n",
        "plt.plot(delta_list, reward_all_c - reward_all_c, label='all_cooperation')\n",
        "plt.legend()\n",
        "plt.xlabel('Discount factor delta')\n",
        "plt.ylabel('Return difference with all_c policy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}